MPI_Bcast	Diffuse une donnée depuis un processus racine vers tous les autres.
MPI_Scatter	Répartit des morceaux de données d’un processus racine vers tous les processus.
MPI_Gather	Collecte des données depuis tous les processus vers le processus racine.
MPI_Allgather	Comme MPI_Gather, mais tous les processus reçoivent les données.
MPI_Barrier	Synchronise tous les processus : aucun ne continue tant que tous n’ont pas atteint la barrière.
MPI_Reduce	Combine les valeurs de tous les processus avec une opération (sum, max, min, etc.) vers un processus racine.
MPI_Allreduce	Comme MPI_Reduce, mais le résultat est disponible pour tous les processus.

== exemples ==

1. MPI_Bcast
#include <mpi.h>
#include <iostream>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    int data;

    if(rank == 0) data = 42; // valeur à diffuser
    MPI_Bcast(&data, 1, MPI_INT, 0, MPI_COMM_WORLD);

    std::cout << "Processus " << rank << " a reçu la valeur " << data << std::endl;

    MPI_Finalize();
    return 0;
}

======================================================
MPI_Scatter
#include <mpi.h>
#include <iostream>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int data[4] = {10, 20, 30, 40};
    int recv;

    MPI_Scatter(data, 1, MPI_INT, &recv, 1, MPI_INT, 0, MPI_COMM_WORLD);

    std::cout << "Processus " << rank << " a reçu " << recv << std::endl;

    MPI_Finalize();
    return 0;
}
======================================================
MPI_Gather
#include <mpi.h>
#include <iostream>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int send = rank * 10;
    int gathered[4];

    MPI_Gather(&send, 1, MPI_INT, gathered, 1, MPI_INT, 0, MPI_COMM_WORLD);

    if(rank == 0) {
        for(int i = 0; i < size; ++i)
            std::cout << "gathered[" << i << "] = " << gathered[i] << std::endl;
    }

    MPI_Finalize();
    return 0;
}
======================================================
MPI_Allgather
#include <mpi.h>
#include <iostream>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int send = rank * 5;
    int allgathered[4];

    MPI_Allgather(&send, 1, MPI_INT, allgathered, 1, MPI_INT, MPI_COMM_WORLD);

    std::cout << "Processus " << rank << " a reçu: ";
    for(int i = 0; i < size; ++i) std::cout << allgathered[i] << " ";
    std::cout << std::endl;

    MPI_Finalize();
    return 0;
}

======================================================
MPI_Barrier

#include <mpi.h>
#include <iostream>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    std::cout << "Processus " << rank << " avant la barrière\n";
    MPI_Barrier(MPI_COMM_WORLD);
    std::cout << "Processus " << rank << " après la barrière\n";

    MPI_Finalize();
    return 0;
}
======================================================
MPI_Reduce


#include <mpi.h>
#include <iostream>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    int value = rank + 1;
    int sum;

    MPI_Reduce(&value, &sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if(rank == 0) std::cout << "Somme totale = " << sum << std::endl;

    MPI_Finalize();
    return 0;
}
======================================================
MPI_Allreduce

#include <mpi.h>
#include <iostream>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    int value = rank + 1;
    int total;

    MPI_Allreduce(&value, &total, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);

    std::cout << "Processus " << rank << " a total = " << total << std::endl;

    MPI_Finalize();
    return 0;
}
